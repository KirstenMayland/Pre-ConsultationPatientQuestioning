# An Evaluation of Lightweight LLM Approaches to Enhancing Pre-Consultation Patient Questioning

Full write-ups and presentations in the WriteUps folder

## Introduction

As artificial intelligence has become prevalent in modern society, its presence in the medical field has been closely looked at as errors and incorrect diagnosis have the potential to result in great harm to someone. In this project, I looked at one aspect of the medical field where artificial intelligence has the possibility in our current state of development to help without the potential for large negative consequences—that of follow-up questions instead of diagnoses. The project looks at language learning model’s capability to prompt a person seeking medical answers for more relevant information to diminish the potential back and forth between a patient and a medical provider—effectively streamlining the pre-consultation phase and supporting better clinical decision-making.

Results showed that while fine-tuning improved question generation, model size remained the most significant factor in performance. Larger models consistently outperformed smaller ones, highlighting the limitations of parameter-constrained LLMs for this task. These findings suggest that while fine-tuning enhances specialized capabilities, robust question generation in medical contexts still demands larger-scale architectures. Future work could explore refining training methodologies and leveraging multimodal data to improve performance in real-world applications.
