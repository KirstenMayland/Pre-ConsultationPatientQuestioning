{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CeF2GZiIgnzt"
      },
      "source": [
        "# Kirsten Mayland - Final Project\n",
        "\n",
        "---\n",
        "\n",
        "Kirsten Mayland (kirsten.r.mayland.25@dartmouth.edu) <br>\n",
        "Dartmouth College, CS72, Winter 2025\n",
        "\n",
        "Purpose: To evaluate the success of our LLMs in creating relevant follow-up questions to medical posts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "zcEpoauAeCZz"
      },
      "outputs": [],
      "source": [
        "# 1 - connect to ChatGPT\n",
        "# 2 - get chatGPT to forecast on the questions as a baseline\n",
        "# 3 - prompt using chain-of-reasoning prompting to ask chatgpt to label the 5 generated sets on five-point likert scale for each of our 4 axes\n",
        "# 4 - in excel? generate means, standard deviation, and total score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zWKLkaf0jhox"
      },
      "source": [
        "## Connect to Dartmouth ChatGPT Api\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "oXeigOd6j2Fe"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install langchain_dartmouth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1MHtFg7mkQSy"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "# os.environ['DARTMOUTH_CHAT_API_KEY'] = ''\n",
        "# os.environ['DARTMOUTH_API_KEY'] = ''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "id": "VRKery1jkaXS"
      },
      "outputs": [],
      "source": [
        "from langchain_dartmouth.llms import DartmouthLLM\n",
        "from langchain_dartmouth.llms import ChatDartmouth\n",
        "from langchain_dartmouth.llms import ChatDartmouthCloud\n",
        "from pprint import pprint\n",
        "import re\n",
        "import pandas as pd\n",
        "from google.colab import files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "id": "rmU0R18onRpg"
      },
      "outputs": [],
      "source": [
        "gpt_4o_mini = ChatDartmouthCloud(model_name=\"openai.gpt-4o-mini-2024-07-18\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dKhjiCKnjpPF"
      },
      "source": [
        "## ChatGPT Baseline Forecast\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "id": "ZDGBW5dRjyUV"
      },
      "outputs": [],
      "source": [
        "def generate_follow_up(title, post):\n",
        "    input_text = f\"Please ask follow up questions to these posts. Your goal is to prompt them to provide more relevant medical information that they might have forgotten to add. Post Title: {title}. Post: {post}\"\n",
        "\n",
        "    # Generate follow-up questions\n",
        "    response = gpt_4o_mini.invoke(input_text)\n",
        "\n",
        "    print(\"Generated:\", response)\n",
        "    print(\"-\" * 40)\n",
        "    return response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "id": "rVF4o-X0napp"
      },
      "outputs": [],
      "source": [
        "# Load CSV\n",
        "# eval_df = pd.read_csv(\"/content/CS72_FinalProject_EvalDataset.csv\", on_bad_lines='skip')\n",
        "# eval_df = eval_df.dropna().reset_index(drop=True)\n",
        "# eval_df['Pre-Training Results'] = eval_df.apply(lambda row: generate_follow_up(row['Title'], row['Post']), axis=1)\n",
        "\n",
        "# eval_df.to_csv(\"/content/CS72_FinalProject_gpt_4o_mini_Results.csv\", index=False, encoding=\"utf-8-sig\")\n",
        "# files.download(\"CS72_FinalProject_gpt_4o_mini_Results.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kr9VmRlCj3OE"
      },
      "source": [
        "## Evaluate Questions\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "id": "SlQkxlmLGQIO"
      },
      "outputs": [],
      "source": [
        "def create_utility_prompt(title, post, follow_up):\n",
        "  prompt = f'''\n",
        "\n",
        "  ### Instruction ###\n",
        "  - Read the title, post, and follow-up questions carefully and determine on a scale from 1-5 with 1 being the least useful and 5 being the most useful, how much utility the follow-up questions have. Utility is defined as how useful the follow-up questions would be to a healthcare provider in responding to the patient message. First, explain your reasoning step by step, then provide the final numeric label.\n",
        "  - Output the label in brackets. E.g. the questions have a utility score of [2]\n",
        "\n",
        "  Now, classify the following post and follow-up questions:\n",
        "  Post Title: {title}\n",
        "  Post: {post}\n",
        "  Follow-up questions: {follow_up}\n",
        "  Reasoning:'''\n",
        "\n",
        "  return prompt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "id": "eXfUmz1_GQ11"
      },
      "outputs": [],
      "source": [
        "def create_necessity_prompt(title, post, follow_up):\n",
        "  prompt = f'''\n",
        "\n",
        "  ### Instruction ###\n",
        "  - Read the title, post, and follow-up questions carefully and determine on a scale from 1-5 with 1 being the least necessary and 5 being the most necessary, how much necessity the follow-up questions have. Necessity is defined as how necessary all of the follow-up questions are for a healthcare provider in addressing the patient’s concern. First, explain your reasoning step by step, then provide the final numeric label.\n",
        "  - Output the label in brackets. E.g. the questions have a necessity score of [2]\n",
        "\n",
        "  Now, classify the following post and follow-up questions:\n",
        "  Post Title: {title}\n",
        "  Post: {post}\n",
        "  Follow-up questions: {follow_up}\n",
        "  Reasoning:'''\n",
        "\n",
        "  return prompt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "id": "EhRd0PE1HoA9"
      },
      "outputs": [],
      "source": [
        "def create_completeness_prompt(title, post, follow_up):\n",
        "  prompt = f'''\n",
        "\n",
        "  ### Instruction ###\n",
        "  - Read the title, post, and follow-up questions carefully and determine on a scale from 1-5 with 1 being the least complete and 5 being the most complete, how much completeness the follow-up questions have. Completeness is defined as how many follow-up questions are not missing important information necessary for a healthcare provider in addressing the patient’s concern. First, explain your reasoning step by step, then provide the final numeric label.\n",
        "  - Output the label in brackets. E.g. the questions have a completeness score of [2]\n",
        "\n",
        "  Now, classify the following post and follow-up questions:\n",
        "  Post Title: {title}\n",
        "  Post: {post}\n",
        "  Follow-up questions: {follow_up}\n",
        "  Reasoning:'''\n",
        "\n",
        "  return prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "id": "dkBTrXyQHpZ4"
      },
      "outputs": [],
      "source": [
        "def create_clarity_prompt(title, post, follow_up):\n",
        "  prompt = f'''\n",
        "clarity is defined as “The follow-up questions are easy to understand and answer by patients.”\n",
        "  ### Instruction ###\n",
        "  - Read the title, post, and follow-up questions carefully and determine on a scale from 1-5 with 1 being the least clear and 5 being the most clear, how much clarity the follow-up questions have. Necessity is defined as how easy the follow-up questions are to understand and answer by patients. First, explain your reasoning step by step, then provide the final numeric label.\n",
        "  - Output the label in brackets. E.g. the questions have a clarity score of [2]\n",
        "\n",
        "  Now, classify the following post and follow-up questions:\n",
        "  Post Title: {title}\n",
        "  Post: {post}\n",
        "  Follow-up questions: {follow_up}\n",
        "  Reasoning:'''\n",
        "\n",
        "  return prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "id": "_GH9HyZsj8Sz"
      },
      "outputs": [],
      "source": [
        "# Chain-of-Thought prompting w/ Special Output Parsing\n",
        "\n",
        "# Regular expression pattern\n",
        "# pattern = r\"\\[(.*?)\\]\"\n",
        "# pprint(response.content)\n",
        "# pprint(re.findall(pattern, response.content))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "id": "XLq_Z13ESaBQ"
      },
      "outputs": [],
      "source": [
        "final = pd.DataFrame()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "id": "czVZA9H7RQN8"
      },
      "outputs": [],
      "source": [
        "def evaluate_follow_up(title, post, follow_up):\n",
        "  utility_prompt = create_utility_prompt(title, post, follow_up)\n",
        "  necessity_prompt = create_necessity_prompt(title, post, follow_up)\n",
        "  completeness_prompt = create_completeness_prompt(title, post, follow_up)\n",
        "  clarity_prompt = create_clarity_prompt(title, post, follow_up)\n",
        "\n",
        "  utility_response = gpt_4o_mini.invoke(utility_prompt)\n",
        "  necessity_response = gpt_4o_mini.invoke(necessity_prompt)\n",
        "  completeness_response = gpt_4o_mini.invoke(completeness_prompt)\n",
        "  clarity_response = gpt_4o_mini.invoke(clarity_prompt)\n",
        "\n",
        "  pattern = r\"\\[(.*?)\\]\"\n",
        "\n",
        "  response_filtered = [re.findall(pattern, utility_response.content) if not None else ['0'], re.findall(pattern, necessity_response.content) if not None else ['0'], re.findall(pattern, completeness_response.content) if not None else ['0'], re.findall(pattern, clarity_response.content) if not None else ['0']]\n",
        "\n",
        "  pprint(response_filtered)\n",
        "  return response_filtered"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "id": "QZ2S_OyaQhOh"
      },
      "outputs": [],
      "source": [
        "flan_base_df = pd.read_csv(\"/content/CS72_FinalProject_flan_t5_base_Results.csv\", on_bad_lines='skip')\n",
        "flan_small_df = pd.read_csv(\"/content/CS72_FinalProject_flan_t5_small_Results.csv\", on_bad_lines='skip')\n",
        "gpt4_df = pd.read_csv(\"/content/CS72_FinalProject_gpt_4o_mini_Results.csv\", on_bad_lines='skip')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "id": "2sZzFzLVQ75B",
        "outputId": "f56047c4-fad6-4b08-d419-a6c91a2a4638"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[['1'], ['1'], ['1'], ['1']]\n",
            "[['1'], ['2'], ['1'], ['2']]\n",
            "[['1'], ['1'], ['1'], ['2']]\n",
            "[['1'], ['2'], ['1'], ['2']]\n",
            "[['1'], ['2'], ['1'], ['2']]\n",
            "[['1'], ['1'], ['1'], ['2']]\n",
            "[['2'], ['3'], ['1'], ['2']]\n",
            "[['1'], ['4'], ['1'], ['2']]\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "{'detail': 'You\\'ve exceeded the daily usage limit (1000 credits) for the paid AI models.\\n                    IMPORTANT: Click the \"New Chat\" button and select one of the free models (ex. Llama 3.1) to start a new chat session.\\n                    '}",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-106-9339a05cbb04>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfinal\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Flan-t5 Base, Pre-training Eval'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflan_base_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mevaluate_follow_up\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Title'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Post'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Pre-training Results'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, axis, raw, result_type, args, by_row, engine, engine_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m  10372\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  10373\u001b[0m         )\n\u001b[0;32m> 10374\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__finalize__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"apply\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m  10375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  10376\u001b[0m     def map(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    914\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_raw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    915\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 916\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    917\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    918\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1061\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1062\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1063\u001b[0;31m             \u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_series_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1064\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1065\u001b[0m             \u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_series_numba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_series_generator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1079\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseries_gen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1080\u001b[0m                 \u001b[0;31m# ignore SettingWithCopy here in case the user mutates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1081\u001b[0;31m                 \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1082\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mABCSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1083\u001b[0m                     \u001b[0;31m# If we have a view on v, we need to make a copy because\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-106-9339a05cbb04>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(row)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfinal\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Flan-t5 Base, Pre-training Eval'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflan_base_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mevaluate_follow_up\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Title'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Post'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Pre-training Results'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-104-01b26058a600>\u001b[0m in \u001b[0;36mevaluate_follow_up\u001b[0;34m(title, post, follow_up)\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0mutility_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgpt_4o_mini\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mutility_prompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0mnecessity_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgpt_4o_mini\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnecessity_prompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m   \u001b[0mcompleteness_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgpt_4o_mini\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompleteness_prompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m   \u001b[0mclarity_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgpt_4o_mini\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclarity_prompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_dartmouth/llms.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    664\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0mrtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mBaseMessage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    665\u001b[0m         \"\"\"\n\u001b[0;32m--> 666\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    667\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    668\u001b[0m     \u001b[0;32masync\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mainvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mBaseMessage\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    305\u001b[0m         return cast(\n\u001b[1;32m    306\u001b[0m             \u001b[0mChatGeneration\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 307\u001b[0;31m             self.generate_prompt(\n\u001b[0m\u001b[1;32m    308\u001b[0m                 \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m                 \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    841\u001b[0m     ) -> LLMResult:\n\u001b[1;32m    842\u001b[0m         \u001b[0mprompt_messages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_messages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprompts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 843\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt_messages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    844\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    845\u001b[0m     async def agenerate_prompt(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_dartmouth/llms.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    686\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mLLMResult\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m     \u001b[0;32masync\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0magenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mLLMResult\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    681\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    682\u001b[0m                 results.append(\n\u001b[0;32m--> 683\u001b[0;31m                     self._generate_with_cache(\n\u001b[0m\u001b[1;32m    684\u001b[0m                         \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    685\u001b[0m                         \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36m_generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    906\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_generate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"run_manager\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 908\u001b[0;31m                 result = self._generate(\n\u001b[0m\u001b[1;32m    909\u001b[0m                     \u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    910\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_openai/chat_models/base.py\u001b[0m in \u001b[0;36m_generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    822\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    823\u001b[0m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mpayload\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 824\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_chat_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgeneration_info\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    825\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    826\u001b[0m     def _get_request_payload(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_openai/chat_models/base.py\u001b[0m in \u001b[0;36m_create_chat_result\u001b[0;34m(self, response, generation_info)\u001b[0m\n\u001b[1;32m    856\u001b[0m         \u001b[0;31m# to mask the true error. Because 'response[\"choices\"]' is None.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresponse_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 858\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    859\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    860\u001b[0m         \u001b[0mtoken_usage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"usage\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: {'detail': 'You\\'ve exceeded the daily usage limit (1000 credits) for the paid AI models.\\n                    IMPORTANT: Click the \"New Chat\" button and select one of the free models (ex. Llama 3.1) to start a new chat session.\\n                    '}"
          ]
        }
      ],
      "source": [
        "final['Flan-t5 Base, Pre-training Eval'] = flan_base_df.apply(lambda row: evaluate_follow_up(row['Title'], row['Post'], row['Pre-training Results']), axis=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jpG_cdnGeQIv"
      },
      "outputs": [],
      "source": [
        "final['Flan-t5 Base, Post-training Eval'] = flan_base_df.apply(lambda row: evaluate_follow_up(row['Title'], row['Post'], row['Post Training Results']), axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c-yGonrteSUZ"
      },
      "outputs": [],
      "source": [
        "final['Flan-t5 Small, Pre-training Eval'] = flan_small_df.apply(lambda row: evaluate_follow_up(row['Title'], row['Post'], row['Pre-training Results']), axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bs8TAmRZeVK0"
      },
      "outputs": [],
      "source": [
        "final['Flan-t5 Small, Post-training Eval'] = flan_small_df.apply(lambda row: evaluate_follow_up(row['Title'], row['Post'], row['Post Training Results']), axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6hSrisLIeWv-"
      },
      "outputs": [],
      "source": [
        "final['GPT-4, Pre-training Eval'] = gpt4_df.apply(lambda row: evaluate_follow_up(row['Title'], row['Post'], row['Pre-Training Results']), axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OuREkwDyULVq"
      },
      "outputs": [],
      "source": [
        "final.to_csv(\"/content/CS72_FinalProject_LLM_Evaluations.csv\", index=False, encoding=\"utf-8-sig\")\n",
        "files.download(\"CS72_FinalProject_LLM_Evaluations.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i8Fa1I3fj9B9"
      },
      "source": [
        "## Run the Numbers\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "enQZvgTIez6R",
        "outputId": "98a39aa6-e9dc-4677-9584-61df27e59a31"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Column: Flan-t5 Base, Pre-training Eval\n",
            "Utility Mean: 1.58, Std Dev: 0.97\n",
            "Necessity Mean: 1.71, Std Dev: 1.00\n",
            "Completeness Mean: 1.17, Std Dev: 0.48\n",
            "Clarity Mean: 1.96, Std Dev: 0.55\n",
            "Overall Mean: 1.60, Overall Std Dev: 0.83\n",
            "----------------------------------------\n",
            "Column: Flan-t5 Base, Post-training Eval\n",
            "Utility Mean: 1.83, Std Dev: 1.31\n",
            "Necessity Mean: 2.08, Std Dev: 1.35\n",
            "Completeness Mean: 1.38, Std Dev: 0.49\n",
            "Clarity Mean: 2.50, Std Dev: 1.44\n",
            "Overall Mean: 1.95, Overall Std Dev: 1.26\n",
            "----------------------------------------\n",
            "Column: Flan-t5 Small, Pre-training Eval\n",
            "Utility Mean: 1.17, Std Dev: 0.38\n",
            "Necessity Mean: 1.21, Std Dev: 0.88\n",
            "Completeness Mean: 1.08, Std Dev: 0.28\n",
            "Clarity Mean: 1.42, Std Dev: 0.50\n",
            "Overall Mean: 1.22, Overall Std Dev: 0.57\n",
            "----------------------------------------\n",
            "Column: Flan-t5 Small, Post-training Eval\n",
            "Utility Mean: 1.21, Std Dev: 0.41\n",
            "Necessity Mean: 1.46, Std Dev: 0.66\n",
            "Completeness Mean: 1.08, Std Dev: 0.41\n",
            "Clarity Mean: 1.58, Std Dev: 0.97\n",
            "Overall Mean: 1.33, Overall Std Dev: 0.68\n",
            "----------------------------------------\n",
            "Column: GPT-4, Pre-training Eval\n",
            "Utility Mean: 2.88, Std Dev: 2.49\n",
            "Necessity Mean: 2.62, Std Dev: 2.48\n",
            "Completeness Mean: 3.75, Std Dev: 2.21\n",
            "Clarity Mean: 4.04, Std Dev: 1.88\n",
            "Overall Mean: 3.32, Overall Std Dev: 2.32\n",
            "----------------------------------------\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import ast\n",
        "\n",
        "final = pd.read_csv(\"/content/CS72_FinalProject_LLM_Evaluations.csv\", on_bad_lines='skip')\n",
        "\n",
        "\n",
        "# Process each column\n",
        "for column in final.columns:\n",
        "  print(f\"Column: {column}\")\n",
        "\n",
        "  utility_scores = []\n",
        "  necessity_scores = []\n",
        "  completeness_scores = []\n",
        "  clarity_scores = []\n",
        "\n",
        "  for row in final[column]:\n",
        "    try:\n",
        "      # Convert string representation of lists into actual lists (if needed)\n",
        "      row = ast.literal_eval(row) if isinstance(row, str) else row\n",
        "\n",
        "      # Ensure row is a list and has at least 4 elements\n",
        "      if isinstance(row, list) and len(row) >= 4:\n",
        "        utility_scores.extend(map(int, row[0]))  # Convert to int and extend list\n",
        "        necessity_scores.extend(map(int, row[1]))\n",
        "        completeness_scores.extend(map(int, row[2]))\n",
        "        clarity_scores.extend(map(int, row[3]))\n",
        "\n",
        "    except Exception as e:\n",
        "      print(f\"Skipping malformed row: {row} due to error {e}\")\n",
        "      continue\n",
        "\n",
        "  # Convert to numpy arrays for easier statistical calculations\n",
        "  utility_scores = np.array(utility_scores) if utility_scores else np.array([0])\n",
        "  necessity_scores = np.array(necessity_scores) if necessity_scores else np.array([0])\n",
        "  completeness_scores = np.array(completeness_scores) if completeness_scores else np.array([0])\n",
        "  clarity_scores = np.array(clarity_scores) if clarity_scores else np.array([0])\n",
        "\n",
        "  # Combine all scores for overall statistics\n",
        "  all_scores = np.concatenate([utility_scores, necessity_scores, completeness_scores, clarity_scores])\n",
        "\n",
        "  # Compute means and standard deviations\n",
        "  print(f\"Utility Mean: {np.mean(utility_scores):.2f}, Std Dev: {np.std(utility_scores, ddof=1):.2f}\")\n",
        "  print(f\"Necessity Mean: {np.mean(necessity_scores):.2f}, Std Dev: {np.std(necessity_scores, ddof=1):.2f}\")\n",
        "  print(f\"Completeness Mean: {np.mean(completeness_scores):.2f}, Std Dev: {np.std(completeness_scores, ddof=1):.2f}\")\n",
        "  print(f\"Clarity Mean: {np.mean(clarity_scores):.2f}, Std Dev: {np.std(clarity_scores, ddof=1):.2f}\")\n",
        "\n",
        "  # Compute overall mean and standard deviation\n",
        "  print(f\"Overall Mean: {np.mean(all_scores):.2f}, Overall Std Dev: {np.std(all_scores, ddof=1):.2f}\")\n",
        "  print(\"-\" * 40)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "dKhjiCKnjpPF"
      ],
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
